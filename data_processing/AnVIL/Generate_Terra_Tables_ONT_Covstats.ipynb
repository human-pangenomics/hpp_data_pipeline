{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Tables For ONT Data QC<a class=\"tocSkip\">\n",
    "\n",
    "**This notebook automatically reads in data stored in the AnVIL_HPRC workspace's bucket and generates data table so the data can be run through QC (NTSM and ReadStat's WDLs)**\n",
    "\n",
    "**Ivo changed this in Sep 2023 to output per flowcell ONT Seq Summary**\n",
    "\n",
    "**Jeltje changed this in Feb 2023 to output per-file tables instead of per-sample**\n",
    "\n",
    "**Below are the steps taken in this notebook:**\n",
    "1. Import Statements & Global Variable Definitions\n",
    "2. Define Functions\n",
    "3. Read In Sample Names\n",
    "4. Create Dataframe Of Files\n",
    "5. Write data frame to data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements & Global Variable Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install gcsfs\n",
    "## capture CANNOT have comments above it\n",
    "## For reading CSVs stored in Google Cloud (without downloading them first)\n",
    "## May need to restart kernel after install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install terra-pandas\n",
    "#%pip install --upgrade --no-cache-dir --force-reinstall terra-pandas\n",
    "%pip install --upgrade --no-cache-dir  --force-reinstall git+https://github.com/DataBiosphere/terra-notebook-utils\n",
    "## For reading/writing data tables into pandas data frames\n",
    "## May need to restart kernel after install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecloud import fiss\n",
    "import pandas as pd         \n",
    "import os                 \n",
    "import subprocess       \n",
    "import re                 \n",
    "import io\n",
    "import gcsfs\n",
    "\n",
    "from typing import Any, Callable, List, Optional\n",
    "from terra_notebook_utils import table, WORKSPACE_NAME, WORKSPACE_GOOGLE_PROJECT\n",
    "from terra_pandas import dataframe_to_table, table_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable Declarations\n",
    "Update this with project specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnVIL_HPRC WorkspaceBucket\n",
    "anvil_hprc_bucket       = \"gs://fc-4310e737-a388-4a10-8c9e-babe06aaf0cf/\"\n",
    "\n",
    "sample_info_fn = 'UCSC_HPRC_ONT_Y3_GUPPY6_samples.tsv' # a list of IDs,e.g. HG01243\n",
    "subm_key       = '79275EDA-C282-424A-9D5B-A8E876592893--UCSC_HPRC_ONT_Y3_GUPPY6'\n",
    "\n",
    "# Get the Google billing project name and workspace name for current workspace\n",
    "PROJECT = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE =os.path.basename(os.path.dirname(os.getcwd()))\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Billing project: \" + PROJECT)\n",
    "print(\"Workspace: \" + WORKSPACE)\n",
    "print(\"Workspace storage bucket: \" + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def rtn_datatype_ls_for_subm(bucket_url, submission_key, data_type_subdir, file_type_ls):\n",
    "    '''Takes in:\n",
    "            * bucket_url (string): url of bucket to search (should be the AnVIL_HPRC bucket)\n",
    "                ex: \"gs://fc-4310e737-a388-4a10-8c9e-babe06aaf0cf/\"\n",
    "            * submission_key (string): the UUID plus the submission name to search\n",
    "                ex: \"5c68b972-8534-402f-9861-11c93558765f--UW_HPRC_HiFi_Y3\"\n",
    "            * data_type_subdir (string): name of the subfolder to search \n",
    "              (used when a submission has more than one data type.)\n",
    "                ex: \"PacBio_HiFi\" if the data is not in subfolders, pass \".\"\n",
    "            * file_type_ls (list of strings): file extensions to search for. Often a submission will \n",
    "              have more than one type of file that represents the same dataset. Be careful to not \n",
    "              include replicate data, however.\n",
    "                ex: \".hifi_reads.bam\"\n",
    "     then performs a list of the bucket, then returns a filtered list files.'''\n",
    "    \n",
    "    rtn_file_ls = []\n",
    "    \n",
    "    submission_path = str(bucket_url + \"submissions/\" + submission_key)\n",
    "    file_list_byt   = subprocess.run(['gsutil', '-u', 'human-pangenome-ucsc', 'ls', '-r', submission_path], \n",
    "                                     stdout=subprocess.PIPE)\n",
    "\n",
    "    file_list_str   = file_list_byt.stdout.decode('utf-8')\n",
    "    file_list       = file_list_str.split('\\n')  ## Pull out \"\\n\"\n",
    "   \n",
    "    ## filter out empty strings\n",
    "    file_list = [ elem for elem in file_list if elem != '']\n",
    "    \n",
    "    ## Pull from subdir type we are targeting\n",
    "    file_list = list(filter(lambda x:re.search(rf\"{data_type_subdir}\", x), file_list))\n",
    "    \n",
    "    for file_type in file_type_ls:\n",
    "    \n",
    "        ## Pull files of correct type (ex: ccs.bam)\n",
    "        file_list_by_type = list(filter(lambda x:re.search(rf\"{file_type}$\", x), file_list))\n",
    "\n",
    "        ## Add to list of files to return\n",
    "        rtn_file_ls += file_list_by_type\n",
    "\n",
    "    return rtn_file_ls    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Sample Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file (a list of sample IDs) should be stored in the Terra workspace (in the bucket) that is being used for \n",
    "## the submission you  are wrangling...\n",
    "sample_info_fp = os.path.join(bucket + sample_info_fn) \n",
    "\n",
    "sample_df = pd.read_csv(sample_info_fp, header=None)\n",
    "\n",
    "sample_df.rename(columns = {0:'sample_id'}, inplace = True)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe Of Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_subdir = \".\"\n",
    "file_type_ls     = [\"sequencing_summary.txt.gz\"]\n",
    "\n",
    "## get a list of the files in submission (subm_key) that match the \n",
    "## data_type_subdir and file_type_ls\n",
    "file_ls  = rtn_datatype_ls_for_subm(anvil_hprc_bucket, subm_key, \n",
    "                                      data_type_subdir, file_type_ls)\n",
    "\n",
    "## Check that the number of files matches what we expect\n",
    "len(file_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Data To File Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedict = dict()\n",
    "\n",
    "for f in file_ls:\n",
    "    for s in sample_df.sample_id:\n",
    "        if re.search(s,f):\n",
    "            filedict[f] = s\n",
    "            break            \n",
    "print(len(filedict))\n",
    "file_df = pd.DataFrame.from_dict(filedict.items())\n",
    "file_df.columns = ['ONT_seq_summ', 'sample']\n",
    "file_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload To Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tables for running NTSM and ReadStats\n",
    "dataframe_to_table(\"covstats\", file_df, WORKSPACE, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
