{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Tables For HiFi Data QC<a class=\"tocSkip\">\n",
    "\n",
    "**This notebook automatically reads in data stored in the AnVIL_HPRC workspace's bucket and generates data table so the data can be run through QC (NTSM and ReadStat's WDL's)**\n",
    "\n",
    "**Note that this notebook requires the following inputs**\n",
    "1. Pedigree file: maps child ID to maternal and paternal IDs. Also used to pull the sample ID from the file key.\n",
    "\n",
    "**Below are the steps taken in this notebook:**\n",
    "1. Import Statements & Global Variable Definitions\n",
    "2. Define Functions\n",
    "3. Read In Sample Names\n",
    "4. Create Dataframe Of Files\n",
    "5. Write data frame to data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements & Global Variable Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For reading CSVs stored in Google Cloud (without downloading them first)\n",
    "## May need to restart kernel after install \n",
    "%%capture\n",
    "%pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For reading/writing data tables into pandas data frames\n",
    "## May need to restart kernel after install \n",
    "%%capture\n",
    "%pip install --upgrade --no-cache-dir --force-reinstall terra-pandas\n",
    "%pip install --upgrade --no-cache-dir  --force-reinstall git+https://github.com/DataBiosphere/terra-notebook-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecloud import fiss\n",
    "import pandas as pd         \n",
    "import os                 \n",
    "import subprocess       \n",
    "import re                 \n",
    "import io\n",
    "import gcsfs\n",
    "\n",
    "from typing import Any, Callable, List, Optional\n",
    "from terra_notebook_utils import table, WORKSPACE_NAME, WORKSPACE_GOOGLE_PROJECT\n",
    "from terra_pandas import dataframe_to_table, table_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnVIL_HPRC WorkspaceBucket\n",
    "anvil_hprc_bucket       = \"gs://fc-4310e737-a388-4a10-8c9e-babe06aaf0cf/\"\n",
    "\n",
    "# Get the Google billing project name and workspace name for current workspace\n",
    "PROJECT = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE =os.path.basename(os.path.dirname(os.getcwd()))\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Billing project: \" + PROJECT)\n",
    "print(\"Workspace: \" + WORKSPACE)\n",
    "print(\"Workspace storage bucket: \" + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtn_datatype_ls_for_subm(bucket_url, submission_key, data_type_subdir, file_type_ls):\n",
    "    '''Takes in:\n",
    "            * bucket_url (string): url of bucket to search (should be the AnVIL_HPRC bucket)\n",
    "                ex: \"gs://fc-4310e737-a388-4a10-8c9e-babe06aaf0cf/\"\n",
    "            * submission_key (string): the UUID plus the submission name to search\n",
    "                ex: \"5c68b972-8534-402f-9861-11c93558765f--UW_HPRC_HiFi_Y3\"\n",
    "            * data_type_subdir (string): name of the subfolder to search \n",
    "              (used when a submission has more than one data type.)\n",
    "                ex: \"PacBio_HiFi\" if the data is not in subfolders, pass \".\"\n",
    "            * file_type_ls (list of strings): file extensions to search for. Often a submission will \n",
    "              have more than one type of file that represents the same dataset. Be careful to not \n",
    "              include replicate data, however.\n",
    "                ex: \".hifi_reads.bam\"\n",
    "     then performs a list of the bucket, then returns a filtered list files.'''\n",
    "    \n",
    "    rtn_file_ls = []\n",
    "    \n",
    "    submission_path = str(bucket_url + \"submissions/\" + submission_key)\n",
    "    file_list_byt   = subprocess.run(['gsutil', '-u', 'firecloud-cgl', 'ls', '-r', submission_path], \n",
    "                                     stdout=subprocess.PIPE)\n",
    "\n",
    "    file_list_str   = file_list_byt.stdout.decode('utf-8')\n",
    "    file_list       = file_list_str.split('\\n')  ## Pull out \"\\n\"\n",
    "   \n",
    "    ## filter out empty strings\n",
    "    file_list = [ elem for elem in file_list if elem != '']\n",
    "    \n",
    "    ## Pull from subdir type we are targeting\n",
    "    file_list = list(filter(lambda x:re.search(rf\"{data_type_subdir}\", x), file_list))\n",
    "    \n",
    "    for file_type in file_type_ls:\n",
    "    \n",
    "        ## Pull files of correct type (ex: ccs.bam)\n",
    "        file_list_by_type = list(filter(lambda x:re.search(rf\"{file_type}$\", x), file_list))\n",
    "\n",
    "        ## Add to list of files to return\n",
    "        rtn_file_ls += file_list_by_type\n",
    "\n",
    "    return rtn_file_ls    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Sample Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file should be stored in the Terra workspace (in the bucket) that is being used for \n",
    "## the submission you  are wrangling...\n",
    "sample_info_fp = \"gs://fc-0de5e548-01c6-4195-a98b-ae7a1953688f/sample_info/UW_HPRC_HiFi_Y3.csv\"\n",
    "\n",
    "sample_df = pd.read_csv(sample_info_fp, header=None)\n",
    "\n",
    "sample_df.rename(columns = {0:'sample_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the sample name to be the index (this is what we want each row to \n",
    "## use as a key -- of sorts -- in Terra)\n",
    "sample_df.set_index('sample_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe Of Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_key         = \"5c68b972-8534-402f-9861-11c93558765f--UW_HPRC_HiFi_Y3\"\n",
    "data_type_subdir = \".\"\n",
    "file_type_ls     = [\".hifi_reads.bam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of the files in submission (subm_key) that match the \n",
    "## data_type_subdir and file_type_ls\n",
    "file_ls  = rtn_datatype_ls_for_subm(anvil_hprc_bucket, subm_key, \n",
    "                                      data_type_subdir, file_type_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that the number of files matches what we expect\n",
    "len(file_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Each Sample's Data To Sample Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data frame to hold data\n",
    "sample_df['hifi'] = ''\n",
    "sample_df['hifi']  = sample_df['hifi'].astype('object')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in sample_df.iterrows():\n",
    "    sample_id = row.name\n",
    "    \n",
    "    sample_file_ls = list(filter(lambda x:re.search(rf\"{sample_id}\", x), file_ls))\n",
    "    sample_df.at[index, \"hifi\"] = sample_file_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take a look to make sure it looks as expected...\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 1000G data (for NTSM run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in 1000G data from another workspace\n",
    "## We will be using this to compare against out submission to make sure that\n",
    "## the data comes from the same samples\n",
    "thousand_genomes_df = table_to_dataframe(\"sample\", \n",
    "                                        workspace=\"1000G-high-coverage-2019\", \n",
    "                                        workspace_namespace=\"anvil-datastorage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the 1kg library name (i.e. HG00621) as the index (that matches our sample name)\n",
    "thousand_genomes_df.set_index('library_name', inplace=True)\n",
    "\n",
    "## We only need the cram file (represents 30X Ilmn dataset)\n",
    "thousand_genomes_df = thousand_genomes_df[['cram']]\n",
    "\n",
    "## name the column to be a bit more descriptive\n",
    "thousand_genomes_df.rename(columns = {'cram':'1000g_cram'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the two dataframes\n",
    "ntsm_df = pd.merge(\n",
    "    sample_df,\n",
    "    thousand_genomes_df,\n",
    "    left_index=True,\n",
    "    right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload To Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tables for running NTSM and ReadStats\n",
    "dataframe_to_table(\"ntsm\",      ntsm_df, WORKSPACE, PROJECT)\n",
    "dataframe_to_table(\"readstats\", sample_df, WORKSPACE, PROJECT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
